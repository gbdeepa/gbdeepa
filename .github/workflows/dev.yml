# This is a basic workflow to help you get started with Actions
        
name: Deploy to Dev
on:
  push:
    branches:
      - develop
  workflow_dispatch:

permissions:
      id-token: write
      contents: read
      
env:
  ENVIRONMENT: dev

jobs:
  build:

    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.8"]

    steps:
      - uses: actions/checkout@v3
      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8 pytest coverage build
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      - name: Lint with flake8
        run: |
          # stop the build if there are Python syntax errors or undefined names
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          # exit-zero treats all errors as warnings. The GitHub editor is 127 chars wide
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      - name: Create Deployment Bucket
        run: |
          #echo "DEPLOYMENT_BUCKET=$(aws sts get-caller-identity --query Account --output text)-us-east-1-deploy" >> $GITHUB_ENV
          echo "DEPLOYMENT_BUCKET= "s3://fj-spark-deploy"  >> $GITHUB_ENV
      - name: Test with pytest
        run: |
          echo "Starting unit tests..."
          coverage run --source=src -m pytest unit
          echo "Finished unit tests..."
      - name: Package
      run: |
        cd ./dist/src
        zip -r ../${{ env.SRC_DIR }}-${{ env.VERSION }}.zip ./*
      - name: Configure AWS credentials from Test account
        uses: aws-actions/configure-aws-credentials@v1
        with:
          role-to-assume: arn:aws:iam::858366249668:role/github-oidc-role   # add IAM role ARN
          aws-region: us-east-1

      - name: Copy files to the test website with the AWS CLI
        run: |
          # download the additional spark jars
          cd dist
          # https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/3.13.27/snowflake-jdbc-3.13.27.jar -P ./
          wget https://repo1.maven.org/maven2/net/snowflake/snowflake-jdbc/${{ env.JDBC_VERSION }}/snowflake-jdbc-{{ env.JDBC_VERSION }}.jar

          # https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_2.13/2.11.1-spark_3.3/spark-snowflake_2.13-2.11.1-spark_3.3.jar
          wget https://repo1.maven.org/maven2/net/snowflake/spark-snowflake_${{ env.SCALA_VERSION }}/${{ env.CONNECTOR_VERSION }}-spark_${{ env.SPARK_VERSION }}/spark-snowflake_${{ env.SCALA_VERSION }}-{{ env.CONNECTOR_VERSION }}-spark_${{ env.SPARK_VERSION }}.jar

          #aws s3 sync . s3://fj-spark-deploy     # add S3 URL 
          aws s3 sync --delete --exact-timestamps . s3://${{ env.DEPLOYMENT_BUCKET }} # /${{ env.NAME }}-glue/${{env.ENVIRONMENT}}
  deploy:
    steps:
      - name: Deploy Cloudformation
        uses: chikin-4x/aws-cloudformation-github-deploy@master
        with:
          name: ${{ env.ENVIRONMENT }}-${{ env.NAME }}-glue
          template: aws/cloudformation.yaml
          capabilities: CAPABILITY_NAMED_IAM
          no-fail-on-empty-changeset: "1"
          parameter-overrides: |
            Name: ${{ env.NAME }}
            Environment: ${{ env.ENVIRONMENT }}
            ProjectBucket: ${{ env.AF_BCKT }}
            DeploymentBucket: ${{ env.DEPLOYMENT_BUCKET }}  # /${{ env.NAME }}-glue/${{env.ENVIRONMENT}}
            # DeltaCoreJarS3Key: delta-core_${{ env.SCALA_VERSION }}-${{ env.DELTA_VERSION }}.jar
            # SparkXMLJarS3Key: spark-xml_${{ env.SCALA_VERSION }}-${{ env.SPARK_XML_VERSION }}.jar
            
          tags: |
            Environment: ${{ env.ENVIRONMENT }}
            GitHubBuildNumber: "${{ github.run_number }}"
            ALLOW_GHA_DELETE: "TRUE"
